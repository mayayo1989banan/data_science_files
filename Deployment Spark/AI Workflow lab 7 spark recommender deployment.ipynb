{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CASE STUDY - Deploying a recommender\n",
    "\n",
    "For this lab we will be using the MovieLens data :\n",
    "\n",
    "* [MovieLens Downloads](https://grouplens.org/datasets/movielens/latest/)\n",
    "\n",
    "The two important pages for documentation are below.\n",
    "\n",
    "* [Spark MLlib collaborative filtering docs](https://spark.apache.org/docs/latest/ml-collaborative-filtering.html) \n",
    "* [Spark ALS docs](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.recommendation.ALS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark as ps\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "DATA_DIR = os.path.join(\"/Users/mayayozhikova/Downloads/Model_Deployment-case-study\", \"data\")\n",
    "SAVE_DIR = os.path.join(\"/Users/mayayozhikova/Downloads/Model_Deployment-case-study\", \"saved-recommender\")\n",
    "\n",
    "if os.path.isdir(SAVE_DIR):\n",
    "    shutil.rmtree(SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/13 15:39:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/11/13 15:39:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "3.3.1\n"
     ]
    }
   ],
   "source": [
    "## ensure the spark context is available\n",
    "spark = (ps.sql.SparkSession.builder\n",
    "        .appName(\"sandbox\")\n",
    "        .getOrCreate()\n",
    "        )\n",
    "\n",
    "sc = spark.sparkContext\n",
    "print(spark.version) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure the data are downloaded, unziped and placed in the data folder of this lab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "movielens_data_dir = os.path.join(DATA_DIR, \"ml-latest-small\")        \n",
    "if not os.path.exists(movielens_data_dir):\n",
    "    print(\"ERROR make sure the path to the Movie Lens data is correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+---------+\n",
      "|user_id|movie_id|rating|timestamp|\n",
      "+-------+--------+------+---------+\n",
      "|      1|       1|   4.0|964982703|\n",
      "|      1|       3|   4.0|964981247|\n",
      "|      1|       6|   4.0|964982224|\n",
      "|      1|      47|   5.0|964983815|\n",
      "+-------+--------+------+---------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## load the ratings data as a pysaprk dataframe\n",
    "ratings_file = os.path.join(movielens_data_dir, \"ratings.csv\")\n",
    "df = spark.read.format(\"csv\").options(header=\"true\", inferSchema=\"true\").load(ratings_file)\n",
    "df = df.withColumnRenamed(\"movieID\", \"movie_id\")\n",
    "df = df.withColumnRenamed(\"userID\", \"user_id\")\n",
    "df.show(n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+\n",
      "|movie_id|               title|              genres|\n",
      "+--------+--------------------+--------------------+\n",
      "|       1|    Toy Story (1995)|Adventure|Animati...|\n",
      "|       2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|       3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|       4|Waiting to Exhale...|Comedy|Drama|Romance|\n",
      "+--------+--------------------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## load the movies data as a pyspark dataframe\n",
    "movies_file = os.path.join(movielens_data_dir, \"movies.csv\") \n",
    "movies_df = spark.read.format(\"csv\").options(header=\"true\", inferSchema=\"true\").load(movies_file)\n",
    "movies_df = movies_df.withColumnRenamed(\"movieID\", \"movie_id\")\n",
    "movies_df.show(n=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Explore the movie lens data a little and summarize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique users:610\n",
      "Unique movies:9742\n",
      "Movies with rating > 2:2983\n",
      "Movies with rating > 3:2442\n",
      "Movies with rating > 4:776\n"
     ]
    }
   ],
   "source": [
    "df_p = df.toPandas()\n",
    "movies_df_p = movies_df.toPandas()\n",
    "df_p.describe()\n",
    "\n",
    "print(f\"Unique users:{df_p['user_id'].nunique()}\")\n",
    "print(f\"Unique movies:{movies_df_p.movie_id.nunique()}\")\n",
    "df_p['rating_mean'] = df_p.groupby('movie_id').apply(lambda x: x['rating'].mean())\n",
    "print(f\"Movies with rating > 2:{df_p[df_p['rating_mean'] > 2].movie_id.nunique()}\")\n",
    "print(f\"Movies with rating > 3:{df_p[df_p['rating_mean'] > 3].movie_id.nunique()}\")\n",
    "print(f\"Movies with rating > 4:{df_p[df_p['rating_mean'] > 4].movie_id.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, user_id: string, movie_id: string, rating: string, timestamp: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Find the ten most popular movies. \n",
    "\n",
    "\n",
    "1. Create 2 pyspark dataframes one with the count of each film in df and one with the average rating of each movie in df.\n",
    "2. Join these two dataframes in a third dataframe. Then, filter this dataframe to select only the movies that have been seen more than 100 times.\n",
    "3. Use the movies_df dataframe to add the names of each movies on the dataframe created in 2. Then, order the dataframe by descending average rating.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------------------+--------------------+--------------------+\n",
      "|movie_id|count|       avg(rating)|               title|              genres|\n",
      "+--------+-----+------------------+--------------------+--------------------+\n",
      "|       1|  215|3.9209302325581397|    Toy Story (1995)|Adventure|Animati...|\n",
      "|       2|  110|3.4318181818181817|      Jumanji (1995)|Adventure|Childre...|\n",
      "|       6|  102| 3.946078431372549|         Heat (1995)|Action|Crime|Thri...|\n",
      "|      10|  132| 3.496212121212121|    GoldenEye (1995)|Action|Adventure|...|\n",
      "|      32|  177| 3.983050847457627|Twelve Monkeys (a...|Mystery|Sci-Fi|Th...|\n",
      "|      34|  128|        3.65234375|         Babe (1995)|      Children|Drama|\n",
      "|      39|  104| 3.293269230769231|     Clueless (1995)|      Comedy|Romance|\n",
      "|      47|  203|3.9753694581280787|Seven (a.k.a. Se7...|    Mystery|Thriller|\n",
      "|      50|  204| 4.237745098039215|Usual Suspects, T...|Crime|Mystery|Thr...|\n",
      "|     110|  237| 4.031645569620253|   Braveheart (1995)|    Action|Drama|War|\n",
      "+--------+-----+------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "## 1_\n",
    "movie_counts = df_p.groupby('movie_id', as_index = False).apply(lambda x: x['user_id'].count())\n",
    "top_rated = df_p.groupby('movie_id', as_index = False).apply(lambda x: x['rating'].mean())\n",
    "\n",
    "## 2_\n",
    "top_movies = pd.merge(movie_counts, top_rated, how = 'inner', on = 'movie_id',suffixes=(\"_x\", \"_y\"))\n",
    "\n",
    "top_movies = top_movies.rename(columns = {'None_x':'count','None_y':'avg(rating)'})\n",
    "top_movies = pd.merge(top_movies, movies_df_p, how = 'inner', on = 'movie_id')\n",
    "\n",
    "## 3_\n",
    "top_movies = top_movies[top_movies['count'] > 100]\n",
    "top_movies = spark.createDataFrame(top_movies)\n",
    "\n",
    "top_movies.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS model\n",
    "\n",
    "We will now fit a ALS model, this is matrix factorization model used for rating recommendation. See the [Spark ALS docs](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.recommendation.ALS)\n",
    "for example usage. \n",
    "\n",
    "First we split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, test) = df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function called **train_model()** that takes two inputs :\n",
    "\n",
    "1. ``reg_param`` : the regularization parameter of the factorization model\n",
    "2. ``implicit_prefs`` : a boolean variable that indicate whereas the model should used explicit or implicit ratings.\n",
    "    \n",
    "The function train an ALS model on the training set then predict the test set and evaluate this prediction.\n",
    "The output of the function should be the RMSE of the fitted model on the test set./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "\n",
    "def train_model(reg_param, implicit_prefs=False):\n",
    "    \"\"\"\n",
    "    Train and evaluate an ALS model\n",
    "    Inputs : the regularization parametre of the ALS model and the implicit_prefs flag\n",
    "    Ouptus : a string with the RMSE and the regularization parameter inputed\n",
    "    \"\"\"\n",
    "    \n",
    "    als = ALS(regParam = reg_param, implicitPrefs = implicit_prefs, userCol = 'user_id', \n",
    "               itemCol = 'movie_id', ratingCol = 'rating', coldStartStrategy = 'drop')\n",
    "    model = als.fit(training)\n",
    "\n",
    "    predictions = model.transform(test)\n",
    "    evaluator = RegressionEvaluator(metricName = 'rmse', labelCol= 'rating', predictionCol = 'prediction')\n",
    "\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    print(\"regParam={}, RMSE={}\".format(reg_param, np.round(rmse, 2)))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the function created above for several ``reg_param`` values find the best regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regParam=0.01, RMSE=1.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regParam=0.05, RMSE=0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regParam=0.1, RMSE=0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regParam=0.15, RMSE=0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regParam=0.25, RMSE=0.9\n"
     ]
    }
   ],
   "source": [
    "for reg_param in [0.01, 0.05, 0.1, 0.15, 0.25]:\n",
    "    train_model(reg_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "With best regParam try using the `implicitPrefs` flag.\n",
    "\n",
    ">Note that the results here make sense because the data are `explicit` ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regParam=0.15, RMSE=3.23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ALSModel: uid=ALS_7ce0acfa248f, rank=10"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(reg_param = 0.15, implicit_prefs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regParam=0.15, RMSE=0.88\n"
     ]
    }
   ],
   "source": [
    "model = train_model(reg_param = 0.15,implicit_prefs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommendation examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2102:=================================================>   (93 + 7) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|user_id|     recommendations|\n",
      "+-------+--------------------+\n",
      "|      1|[{170355, 5.76992...|\n",
      "|      3|[{70946, 4.874582...|\n",
      "|      5|[{132333, 4.67290...|\n",
      "|      6|[{3153, 5.0950294...|\n",
      "+-------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_recs = model.recommendForAllUsers(10)\n",
    "user_recs.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2151:==================================================>  (96 + 4) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|movie_id|     recommendations|\n",
      "+--------+--------------------+\n",
      "|      26|[{43, 4.9194374},...|\n",
      "|      27|[{543, 5.331378},...|\n",
      "|      28|[{53, 5.536298}, ...|\n",
      "|      31|[{53, 4.579166}, ...|\n",
      "+--------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "movie_recs = model.recommendForAllItems(10)\n",
    "movie_recs.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrain the model with best ``reg_param`` and ``implicit_prefs`` on the entire dataset and save the trained model in the SAVE_DIR directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...training\n",
      "...saving als model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2255:>                                                      (0 + 8) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/13 17:09:12 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "22/11/13 17:09:12 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "22/11/13 17:09:12 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE (Replace the symbole #<> with your code)\n",
    "\n",
    "## re-train using the whole data set\n",
    "print(\"...training\")\n",
    "als = ALS(regParam = 0.15, implicitPrefs = False, userCol = 'user_id', \n",
    "               itemCol = 'movie_id', ratingCol = 'rating', coldStartStrategy = 'drop')\n",
    "model = als.fit(training)\n",
    " \n",
    "## save model\n",
    "print(\"...saving als model\")\n",
    "model.save(SAVE_DIR)  \n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to use ``spark-submit`` to load the model and demonstrate that you can load the model and interface with it.\n",
    "\n",
    "Following the best practices we created a python script (``recommender-submit.py``) in the **scripts** folder that loads the model, creates some hand crafted data points and query the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/11/13 17:15:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/11/13 17:15:31 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/11/13 17:15:31 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "best rated [(260,), (2628,), (1196,), (122886,), (187595,), (179819,), (1210,)]\n",
      "closest_users                                                                   \n",
      " [(53,), (543,), (276,), (452,), (99,), (93,), (97,), (475,), (515,), (30,), (1,), (523,), (389,), (12,), (106,), (25,), (122,), (154,), (267,), (371,), (169,), (340,), (52,), (69,), (246,), (80,), (573,), (450,), (574,), (519,), (435,), (491,), (336,), (31,), (171,), (527,), (348,), (224,), (327,), (486,), (413,), (186,), (46,), (585,), (45,), (337,), (555,), (459,), (220,), (601,), (540,), (43,), (360,), (243,), (453,), (58,), (586,), (511,), (11,), (594,), (364,), (92,), (119,), (592,), (213,), (251,), (95,), (269,), (35,), (319,), (553,), (162,), (382,), (176,), (578,), (472,), (210,), (408,), (388,), (558,), (48,), (37,), (579,), (112,), (291,), (482,), (206,), (201,), (533,), (595,), (273,), (597,), (400,), (79,), (164,), (417,), (377,), (447,), (562,), (538,), (367,), (495,), (138,), (554,), (589,), (62,), (355,), (441,), (49,), (192,), (344,), (569,), (155,), (380,), (256,), (537,), (544,), (77,), (458,), (466,), (236,), (59,), (572,), (250,), (465,), (252,), (275,), (505,), (568,), (302,), (499,), (375,), (362,), (532,), (460,), (234,), (587,), (548,), (430,), (341,), (492,), (108,), (240,), (303,), (225,), (556,), (304,), (539,), (584,), (239,), (284,), (200,), (115,), (551,), (451,), (40,), (90,), (70,), (290,), (66,), (51,), (500,), (429,), (259,), (335,), (582,), (158,), (161,), (550,), (352,), (345,), (456,), (100,), (484,), (591,), (71,), (393,), (196,), (488,), (280,), (55,), (399,), (147,), (531,), (21,), (498,), (82,), (383,), (477,), (299,)]\n"
     ]
    }
   ],
   "source": [
    "! python /Users/mayayozhikova/Downloads/Model_Deployment-case-study/scripts/recommender-submit.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
